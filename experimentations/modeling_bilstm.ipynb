{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from Masakhane folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas keras plot_keras_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auglib import augment_ner_iob_data\n",
    "from auglib import read_format_iob_data, list_to_pd_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading folder path\n",
    "# Context Masahkane community\n",
    "# Ramshaw and Marcus (1995) = IOB data format\n",
    "\n",
    "bbj_ner_path = Path('../data_source/masakhane-ner/MasakhaNER2.0/data/bbj')\n",
    "dev_data_path   = bbj_ner_path / 'dev.txt'\n",
    "train_data_path = bbj_ner_path / 'train.txt'\n",
    "test_data_path  = bbj_ner_path / 'test.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and extract iob data\n",
    "extracted_train_data, pd_train_data, only_train_ner_data, o_train_ner_data = read_format_iob_data(train_data_path)\n",
    "extracted_test_data, pd_test_data, only_test_ner_data, o_test_ner_data = read_format_iob_data(test_data_path)\n",
    "extracted_dev_data, pd_dev_data, only_dev_ner_data, o_dev_ner_data = read_format_iob_data(dev_data_path)\n",
    "\n",
    "# quelques stats liminaires\n",
    "print(\"Total number of sentences in the train dataset: {:,}\".format(pd_train_data[\"sentence_id\"].nunique()))\n",
    "print(\"Total words in the train dataset: {:,}\".format(pd_train_data.shape[0]))\n",
    "print(\"Total number of sentences in the test dataset: {:,}\".format(pd_test_data[\"sentence_id\"].nunique()))\n",
    "print(\"Total words in the test dataset: {:,}\".format(pd_test_data.shape[0]))\n",
    "print(\"Total number of sentences in the dev dataset: {:,}\".format(pd_dev_data[\"sentence_id\"].nunique()))\n",
    "print(\"Total words in the dev dataset: {:,}\".format(pd_dev_data.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_ner_data = augment_ner_iob_data(only_train_ner_data)\n",
    "\n",
    "print(f' Train data: {len(only_train_ner_data)} \\n Augmented data: {len(augmented_train_ner_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to pd format\n",
    "pd_augment_ner_iob_data = list_to_pd_format(augmented_train_ner_data)\n",
    "pd_augment_ner_iob_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantification des tags sur le corpus d'entrainement\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd_augment_ner_iob_data[pd_augment_ner_iob_data.iob_tag != 'O'][\"iob_tag\"]\\\n",
    ".value_counts().plot(kind=\"bar\", figsize=(20,15))\n",
    "\n",
    "# On remarque ici que le tag date représente 40% des tags (en position I ou B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_counts = pd_augment_ner_iob_data.groupby(\"sentence_id\")[\"word\"].agg([\"count\"])\n",
    "word_counts = word_counts.rename(columns={\"count\": \"Word count\"})\n",
    "word_counts.hist(bins=15, figsize=(8,6))\n",
    "\n",
    "# On constate que le nombre de mots moyens par phrase est de 15\n",
    "MAX_LENGTH_SENTENCE = word_counts.max()[0]\n",
    "print(\"La phrase la plus longue contient {} mots.\".format(MAX_LENGTH_SENTENCE))\n",
    "longest_sentence_id = word_counts[word_counts[\"Word count\"]==MAX_LENGTH_SENTENCE].index[0]\n",
    "print(\"ID de la plus longue phrase est: {}.\".format(longest_sentence_id))\n",
    "longest_sentence = pd_augment_ner_iob_data[pd_augment_ner_iob_data[\"sentence_id\"]==longest_sentence_id][\"word\"].str.cat(sep=' ')\n",
    "print(f\"\\nLa phrase la plus longue du corpus est: \\n {longest_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set(pd_augment_ner_iob_data[\"word\"].values))\n",
    "all_tags = list(set(pd_augment_ner_iob_data[\"iob_tag\"].values))\n",
    "\n",
    "print(\"Nombre de mots uniques: {}\".format(pd_augment_ner_iob_data[\"word\"].nunique()))\n",
    "print(\"Nombre de tags uniques : {}\".format(pd_augment_ner_iob_data[\"iob_tag\"].nunique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to index\n",
    "word2index = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
    "word2index[\"--UNKNOWN_WORD--\"]=0\n",
    "word2index[\"--PADDING--\"]=1\n",
    "index2word = {idx: word for word, idx in word2index.items()}\n",
    "# tag to index\n",
    "tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "tag2index[\"--PADDING--\"]=1\n",
    "index2tag = {idx: word for word, idx in tag2index.items()}\n",
    "\n",
    "# test for one word\n",
    "test_word = \"André\"\n",
    "test_word_idx = word2index[test_word]\n",
    "test_word_lookup = index2word[test_word_idx]\n",
    "print(\"L'index du mot {} est {}.\".format(test_word, test_word_idx))\n",
    "print(\"Le mot avec l'index {} est {}.\".format(test_word_idx, test_word_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_ner_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "X_words = [[word[0] for word in sentence] for sentence in augmented_train_ner_data]\n",
    "y_tags = [[word[1] for word in sentence] for sentence in augmented_train_ner_data]\n",
    "print(\"X_words[10]:\", X_words[10])\n",
    "print(\"y_tags[10]:\", y_tags[10])\n",
    "\n",
    "X_words = [[word2index[word] for word in sentence] for sentence in X_words]\n",
    "y_tags = [[tag2index[tag] for tag in sentence] for sentence in y_tags]\n",
    "print(\"\\nword2index - X_words[10]:\", X_words[10])\n",
    "print(\"tag2index - y_tags[10]:\", y_tags[10])\n",
    "\n",
    "#X_words = [sentence + [word2index[\"--PADDING--\"]] * (MAX_LENGTH_SENTENCE - len(sentence)) for sentence in X_words]\n",
    "#y_tags = [sentence + [tag2index[\"--PADDING--\"]] * (MAX_LENGTH_SENTENCE - len(sentence)) for sentence in y_tags]\n",
    "X_words = pad_sequences(X_words, maxlen=MAX_LENGTH_SENTENCE, padding='post', value=word2index[\"--PADDING--\"])\n",
    "y_tags = pad_sequences(y_tags, maxlen=MAX_LENGTH_SENTENCE, padding='post', value=tag2index[\"--PADDING--\"])\n",
    "\n",
    "\n",
    "print(\"\\npadding - X_words[10]: \", len(X_words[10]),  X_words[10])\n",
    "print(\"padding - y_tags[10]: \", len(y_tags[10]), y_tags[10])\n",
    "\n",
    "TAG_COUNT = len(tag2index)\n",
    "y_tags = [np.eye(TAG_COUNT)[sentence] for sentence in y_tags]\n",
    "# [np.eye(TAG_COUNT)[sentence] for sentence in augmented_train_ner_data]\n",
    "# to_categorical(y_tags, num_classes=TAG_COUNT)\n",
    "\n",
    "print(\"to categorical - y_tags[10]:\", y_tags[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_tags), len(X_words))\n",
    "\n",
    "y_train = np.array(y_tags)\n",
    "X_train = np.array(X_words)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall keras tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install plot_keras_history keras tensorflow_addons tensorflow\n",
    "!pip3 install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keras_preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall keras-contrib -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_CPP_MIN_LOG_LEVEL=\"2\"\n",
    "!sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda11.0 \\\n",
    "    libnvinfer-dev=6.0.1-1+cuda11.0 \\\n",
    "    libnvinfer-plugin6=6.0.1-1+cuda11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from plot_keras_history import plot_history\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from keras_contrib.utils import save_load_utils\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras import Input\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras_contrib.layers import CRF\n",
    "#from tensorflow_addons.layers import CRF\n",
    "from keras_contrib import losses\n",
    "from keras_contrib import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_COUNT = len(index2word)\n",
    "DENSE_EMBEDDING = 50\n",
    "LSTM_UNITS = 10\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_UNITS = 20\n",
    "BATCH_SIZE = 20\n",
    "MAX_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=(MAX_LENGTH_SENTENCE,))\n",
    "\n",
    "model = layers.Embedding(WORD_COUNT, DENSE_EMBEDDING, embeddings_initializer=\"uniform\", \n",
    "                                input_length=MAX_LENGTH_SENTENCE)(input_layer)\n",
    "model = layers.Bidirectional(layers.LSTM(LSTM_UNITS, recurrent_dropout=LSTM_DROPOUT, \n",
    "                            return_sequences=True))(model)\n",
    "model = layers.TimeDistributed(layers.Dense(DENSE_UNITS, activation=\"relu\"))(model)\n",
    "\n",
    "crf_layer = CRF(units=TAG_COUNT, sparse_target=True)\n",
    "output_layer = crf_layer(model)\n",
    "\n",
    "ner_model = Model(input_layer, output_layer)\n",
    "\n",
    "loss = losses.crf_loss\n",
    "acc_metric = metrics.crf_accuracy\n",
    "opt = optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "ner_model.compile(optimizer=opt, loss=loss, metrics=[acc_metric])\n",
    "ner_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=acc_metric, verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ner_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, \n",
    "                        verbose=2, callbacks=callbacks_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia-crf-lf-OXAjte5Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "646b439550bec9cacc5e0384422c9ee78f8df74b182cfe1fc7410e07b34d6961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
