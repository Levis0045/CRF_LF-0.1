{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sangkak AI Challenge: NER tasks\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author**: Elvis MBONING (NTeALan Research and Development Team)\n",
    "- **Session**: février 2023\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we try to evaluate models build with CRF on test data. Depending of the type of model, we will give specific evaluation.\n",
    "\n",
    "We want to evaluate these points:\n",
    "\n",
    " - the F1 score of each label in the model\n",
    " - the impact of augmentation on the model\n",
    " - the impact of selected features on the model\n",
    " - the capacity of model to give good tag in new dataset\n",
    " - comparaison between the final best model with results of Masahkane\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python packages dependencies (if not already installed)\n",
    "!pip3 install pandas python_crfsuite summarytools sklearn_crfsuite unidecode\n",
    "!pip3 install iteration_utilities matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbj_ner_path = Path('../data_source/masakhane-ner/MasakhaNER2.0/data/bbj')\n",
    "dev_data_path   = bbj_ner_path / 'dev.txt'\n",
    "\n",
    "def check_ner_type(ner_data):\n",
    "    if 'I-DATE' in ner_data or 'B-LOC' in ner_data or 'B-PER' in ner_data \\\n",
    "        or 'I-PER' in ner_data or 'B-DATE' in ner_data or 'B-ORG' in ner_data \\\n",
    "        or 'I-ORG' in ner_data or 'I-LOC' in ner_data:\n",
    "        return True\n",
    "    else: return False\n",
    "    \n",
    "# function that read IOB file and build data structure for train, test and dev\n",
    "def read_format_iob_data(filename):\n",
    "    sents_id, words, iob_tag = [], [], []\n",
    "    all_extracted_data, only_ner_data, o_ner_data = [], [], []\n",
    "    with open(filename, encoding='utf-8') as iob:\n",
    "        sentence, id_sent, tags = [], 1, []\n",
    "        for line in iob:\n",
    "            if len(line) > 1:\n",
    "                word, tag = line.strip().split(' ')\n",
    "                sentence.append((word, tag))\n",
    "                sents_id.append(id_sent)\n",
    "                words.append(word)\n",
    "                iob_tag.append(tag)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                if sentence[-1] != '.': \n",
    "                    sentence.append(('.', 'O'))\n",
    "                    words.append('.')\n",
    "                    iob_tag.append('O')\n",
    "                sents_id.append(id_sent)\n",
    "                all_extracted_data.append(sentence)\n",
    "                if check_ner_type(tags): only_ner_data.append(sentence)\n",
    "                else: o_ner_data.append(sentence)\n",
    "                sentence = []\n",
    "                id_sent += 1\n",
    "                tags = []\n",
    "    dataframe = {\"sentence_id\": sents_id, \"word\": words, \"iob_tag\": iob_tag}\n",
    "    pd_iob_data = pd.DataFrame.from_dict(dataframe)\n",
    "    return all_extracted_data, pd_iob_data, only_ner_data, o_ner_data\n",
    "\n",
    "\n",
    "extracted_train_data, pd_train_data, only_train_ner_data, o_train_ner_data = read_format_iob_data(dev_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iteration_utilities import unique_everseen\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_iob_group_entities(data, output_format='dict', remove_duplicates=False):\n",
    "    results = []\n",
    "    match_group, b, i = [], False, False\n",
    "    for w in data:\n",
    "        if w[1].startswith('B') and not i and not b: \n",
    "            match_group.append(w)\n",
    "            b = True\n",
    "        elif w[1].startswith('I') and b and not i:\n",
    "            match_group.append(w)\n",
    "            i = True\n",
    "            b = False\n",
    "        elif w[1].startswith('I') and not b:\n",
    "            match_group.append(w)\n",
    "            i = True\n",
    "        elif w[1].startswith('B') and i and not b: \n",
    "            results.append(match_group)\n",
    "            match_group, b, i = [], True, False\n",
    "            match_group.append(w)\n",
    "        elif w[1].startswith('B') and not i and b: \n",
    "            results.append(match_group)\n",
    "            match_group = []\n",
    "            match_group.append(w)            \n",
    "        else:\n",
    "            print('----------------', w)\n",
    "\n",
    "    if remove_duplicates:\n",
    "        results = list(unique_everseen(results, key=list))\n",
    "\n",
    "    if output_format == 'dict':\n",
    "        results = [dict(x) for x in results]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_iob_tag_position(iob_sent): \n",
    "    results = []\n",
    "    match_group, b, i = [], False, False\n",
    "    for id, w in enumerate(iob_sent):\n",
    "        v = list(w)\n",
    "        if w[1].startswith('B') and not i and not b: \n",
    "            v.append(id)\n",
    "            match_group.append(v)\n",
    "            b = True\n",
    "        elif w[1].startswith('I') and b and not i:\n",
    "            v.append(id)\n",
    "            match_group.append(v)\n",
    "            i = True\n",
    "            b = False\n",
    "        elif w[1].startswith('I') and not b:\n",
    "            v.append(id)\n",
    "            match_group.append(v)\n",
    "            i = True\n",
    "        elif w[1].startswith('B') and i and not b: \n",
    "            results.append(match_group)\n",
    "            match_group, b, i = [], True, False\n",
    "            v.append(id)\n",
    "            match_group.append(v)\n",
    "        elif w[1].startswith('B') and not i and b: \n",
    "            results.append(match_group)\n",
    "            match_group = []\n",
    "            v.append(id)\n",
    "            match_group.append(v)\n",
    "        if len(iob_sent)==id+1 and len(match_group) == 1 and len(results) == 0:\n",
    "            results.append(match_group)\n",
    "        elif len(iob_sent)==id+1: results.append(match_group)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_iob_type(data):\n",
    "    out = [x[1] for x in data][0].split('-')[1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def augment_sentence(sentence, list_ent_aug=None):\n",
    "    # pour la phrase en entrée, générer n phrases supplémentaires à partir \n",
    "    # des entités fournies\n",
    "    word_positions = detect_iob_tag_position(sentence)\n",
    "    results_aug = []\n",
    "    #print('\\n=> ', \" \".join([i[0] for i in sentence]), '---', word_positions, end='\\n\\n')\n",
    "    for word in word_positions:\n",
    "        tag = detect_iob_type(word)\n",
    "        sent_aug = []\n",
    "        for entity in list_ent_aug[tag]:\n",
    "            pos = [x[2] for x in word]\n",
    "            for i, x in enumerate(sentence):\n",
    "                if i not in pos: sent_aug.append(x)\n",
    "                else: \n",
    "                    #print(x)\n",
    "                    if entity[0] not in sent_aug and word not in entity: \n",
    "                        for e in entity: sent_aug.append(e)\n",
    "\n",
    "            if sent_aug not in results_aug: \n",
    "                results_aug.append(sent_aug)\n",
    "            sent_aug = []\n",
    "                \n",
    "            #print('---', word, '---', pos, ' --- ',entity)\n",
    "            #print('\\n---> ', results_aug)\n",
    "    return results_aug\n",
    "\n",
    "\n",
    "def augment_ner_iob_data(train_data, normalize=False):\n",
    "    \"\"\"Position to position augmentation: generate alternate sentence base on \n",
    "    entities position of the sentence and all others entities groups\n",
    "\n",
    "    Args:\n",
    "        train_data (list): list of input sentences in iob format\n",
    "\n",
    "    Returns:\n",
    "        list: list of generate alternate sentences\n",
    "    \"\"\"\n",
    "    org_list  = [x for sent in train_data for x in sent if x[1] in ['B-ORG','I-ORG']]\n",
    "    date_list = [x for sent in train_data for x in sent if x[1] in ['B-DATE','I-DATE']]\n",
    "    loc_list  = [x for sent in train_data for x in sent if x[1] in ['B-LOC','I-LOC']]\n",
    "    per_list  = [x for sent in train_data for x in sent if x[1] in ['B-PER','I-PER']]\n",
    "    \n",
    "    org_list_group  = extract_iob_group_entities(org_list, output_format='list', remove_duplicates=True)\n",
    "    date_list_group = extract_iob_group_entities(date_list, output_format='list', remove_duplicates=True)\n",
    "    loc_list_group  = extract_iob_group_entities(loc_list, output_format='list', remove_duplicates=True)\n",
    "    per_list_group  = extract_iob_group_entities(per_list, output_format='list', remove_duplicates=True)\n",
    "\n",
    "    ents_groups = {'ORG': org_list_group, 'LOC': loc_list_group, \n",
    "        'PER': per_list_group, 'DATE': date_list_group\n",
    "    }\n",
    "\n",
    "    augment_sentences_train = train_data.copy()\n",
    "    for sentence in train_data:\n",
    "        results_augment = augment_sentence(sentence, list_ent_aug=ents_groups)\n",
    "        augment_sentences_train = results_augment + augment_sentences_train\n",
    "\n",
    "    return augment_sentences_train\n",
    "    \n",
    "\n",
    "def list_to_pd_format(data):\n",
    "    sent_id, ids, words, tags = 1, [], [], []\n",
    "    for sent in data:\n",
    "        for word in sent:\n",
    "            if word[0] != '.':\n",
    "                ids.append(sent_id)\n",
    "                words.append(word[0])\n",
    "                tags.append(word[1])\n",
    "            else:\n",
    "                ids.append(sent_id)\n",
    "                words.append(word[0])\n",
    "                tags.append(word[1])\n",
    "                sent_id += 1\n",
    "                \n",
    "    dataframe = {\"sentence_id\": ids, \"word\": words, \"iob_tag\": tags}\n",
    "    pd_iob_data = pd.DataFrame.from_dict(dataframe)\n",
    "    return pd_iob_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(org_list[0:12])\n",
    "#org_list_group   = extract_iob_group_entities(org_list, output_format='list', remove_duplicates=True)\n",
    "#date_list_group = extract_iob_group_entities(date_list)\n",
    "#loc_list_group  = extract_iob_group_entities(loc_list)\n",
    "#per_list_group  = extract_iob_group_entities(per_list)\n",
    "#org_list_group[:12]\n",
    "#print(only_train_ner_data[10])\n",
    "\n",
    "\n",
    "data_extract = [('Mdyə̂faʼ', 'O'), ('mtəŋláʼ', 'B-LOC'), ('shyə̂ŋkaʼ', 'I-LOC'), (',', 'O'), \n",
    "('təŋláʼ', 'B-LOC'), ('ŋkaʼ', 'I-LOC'), ('gə́', 'O'), ('təŋláʼ', 'O'), \n",
    "('Adamáwǎ', 'B-LOC'), ('kuʼ', 'O'), ('dəŋ', 'O'), ('é', 'O'), ('.', 'O'), ('.', 'O')]\n",
    "#detect_iob_tag_position(data_extract)\n",
    "\n",
    "#results_augment = augment_sentence(sent, list_ent_aug=ents_groups)\n",
    "#for sent in results_augment: print('\\t', \" \".join([i[0] for i in sent]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia-crf-lf-OXAjte5Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "646b439550bec9cacc5e0384422c9ee78f8df74b182cfe1fc7410e07b34d6961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
