{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Levis0045/SCIA-CRF_LF/blob/0.1/training/experimentations_crf_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XKvm_OK-Oqx"
      },
      "source": [
        "# Loading data from Masakhane folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uDW7Mxx1-RoP",
        "outputId": "9eac6c2c-417c-4f9a-f347-81eb25017b8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "7MGxSaAP-Oq0",
        "outputId": "dbed5158-e372-4410-9db9-1225d6263abc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Collecting python_crfsuite\n",
            "  Downloading python_crfsuite-0.9.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting summarytools\n",
            "  Downloading summarytools-0.2.3.tar.gz (11 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Collecting ipython>=7.20.0\n",
            "  Downloading ipython-8.10.0-py3-none-any.whl (784 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m784.3/784.3 KB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib>=3.3.0\n",
            "  Downloading matplotlib-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (4.64.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=7.20.0->summarytools) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.20.0->summarytools) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.20.0->summarytools) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=7.20.0->summarytools) (0.2.0)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.30\n",
            "  Downloading prompt_toolkit-3.0.37-py3-none-any.whl (385 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 KB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=7.20.0->summarytools) (0.7.5)\n",
            "Collecting stack-data\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.20.0->summarytools) (2.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->summarytools) (23.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->summarytools) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->summarytools) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->summarytools) (5.10.2)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->summarytools) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.0->summarytools) (7.1.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->summarytools) (3.13.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=7.20.0->summarytools) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=7.20.0->summarytools) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=7.20.0->summarytools) (0.2.6)\n",
            "Collecting pure-eval\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting executing>=1.2.0\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.1.0\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: summarytools\n",
            "  Building wheel for summarytools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summarytools: filename=summarytools-0.2.3-py3-none-any.whl size=8597 sha256=05364159f070ecf7b07eae9dca703eedd388c6cd13d0f6058697fed7dc34a007\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/bb/57/3cb5f446df605f08cfd3f06d3b296d8a2fc7a4868033d4561f\n",
            "Successfully built summarytools\n",
            "Installing collected packages: python_crfsuite, pure-eval, executing, unidecode, sklearn_crfsuite, prompt-toolkit, matplotlib-inline, jedi, fonttools, contourpy, asttokens, stack-data, pandas, matplotlib, ipython, summarytools\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 2.0.10\n",
            "    Uninstalling prompt-toolkit-2.0.10:\n",
            "      Successfully uninstalled prompt-toolkit-2.0.10\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.9.0\n",
            "    Uninstalling ipython-7.9.0:\n",
            "      Successfully uninstalled ipython-7.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-2.2.1 contourpy-1.0.7 executing-1.2.0 fonttools-4.38.0 ipython-8.10.0 jedi-0.18.2 matplotlib-3.7.0 matplotlib-inline-0.1.6 pandas-1.5.3 prompt-toolkit-3.0.37 pure-eval-0.2.2 python_crfsuite-0.9.9 sklearn_crfsuite-0.3.6 stack-data-0.6.2 summarytools-0.2.3 unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pandas python_crfsuite summarytools sklearn_crfsuite unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ToEPGVTh-Oq1",
        "outputId": "833cbaa5-05fe-440b-8d5f-60d25cbbdebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting iteration_utilities\n",
            "  Downloading iteration_utilities-0.11.0-cp38-cp38-manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.6/344.6 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.7.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (5.10.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Installing collected packages: iteration_utilities\n",
            "Successfully installed iteration_utilities-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install iteration_utilities matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/masakhane-io/masakhane-ner.git"
      ],
      "metadata": {
        "id": "_BnXAElK-9KU",
        "outputId": "dec1b3a8-7951-4df6-fd85-ce95a0e03f99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'masakhane-ner'...\n",
            "remote: Enumerating objects: 3923, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 3923 (delta 78), reused 103 (delta 50), pack-reused 3734\u001b[K\n",
            "Receiving objects: 100% (3923/3923), 54.21 MiB | 20.28 MiB/s, done.\n",
            "Resolving deltas: 100% (1666/1666), done.\n",
            "Updating files: 100% (2133/2133), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "OW5yryND-Oq1"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKkOHE-Y-Oq2"
      },
      "source": [
        "- normalization des données d'entrainement et application de quelques corrections\n",
        "  - ajout de point à la fin des phrases qui n'en ont pas \n",
        "  - considérer les tirets comme partie de \"I-LOC\" ?\n",
        "  - ŋkaʼ (6) est à la fois O et B-DATE+chiffre (37)\n",
        "  - non standardisation de l'orthographe: \n",
        "        Afrika I-ORG  / Afríkà B-LOC / Afrikǎ I-LOC / Afrika B-LOC / Afrika I-ORG\n",
        "        / afika B-LOC / Afríka B-LOC / Africa I-ORG / Afrika O / Afríkâ B-LOC\n",
        "\n",
        "      Nəmo B-ORG / NƏMO O / Nə̀mò B-ORG\n",
        "\n",
        "- construire une stratégie d'augmentation sur les entités existantes\n",
        "  - position to position augmentation\n",
        "  - \n",
        "\n",
        "\n",
        "- construire un classifier avec "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "id": "fQbKTIOV-Oq2"
      },
      "outputs": [],
      "source": [
        "# Reading folder path\n",
        "# Context Masahkane community\n",
        "# Ramshaw and Marcus (1995) = IOB data format\n",
        "\n",
        "bbj_ner_path = Path('./masakhane-ner/MasakhaNER2.0/data/bbj')\n",
        "dev_data_path   = bbj_ner_path / 'dev.txt'\n",
        "train_data_path = bbj_ner_path / 'train.txt'\n",
        "test_data_path  = bbj_ner_path / 'test.txt'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4GFibxHR-Oq3"
      },
      "outputs": [],
      "source": [
        "from auglib import augment_ner_iob_data\n",
        "from auglib import read_format_iob_data, list_to_pd_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "sbODGde7-Oq3",
        "outputId": "475740c0-3847-4365-c7f3-d3d44f861e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences in the train dataset: 3,384\n",
            "Total words in the train dataset: 50,623\n",
            "Total number of sentences in the test dataset: 966\n",
            "Total words in the test dataset: 15,924\n",
            "Total number of sentences in the dev dataset: 483\n",
            "Total words in the dev dataset: 7,745\n"
          ]
        }
      ],
      "source": [
        "# read and extract iob data\n",
        "extracted_train_data, pd_train_data, only_train_ner_data, o_train_ner_data = read_format_iob_data(train_data_path)\n",
        "extracted_test_data, pd_test_data, only_test_ner_data, o_test_ner_data = read_format_iob_data(test_data_path)\n",
        "extracted_dev_data, pd_dev_data, only_dev_ner_data, o_dev_ner_data = read_format_iob_data(dev_data_path)\n",
        "\n",
        "# quelques stats liminaires\n",
        "print(\"Total number of sentences in the train dataset: {:,}\".format(pd_train_data[\"sentence_id\"].nunique()))\n",
        "print(\"Total words in the train dataset: {:,}\".format(pd_train_data.shape[0]))\n",
        "print(\"Total number of sentences in the test dataset: {:,}\".format(pd_test_data[\"sentence_id\"].nunique()))\n",
        "print(\"Total words in the test dataset: {:,}\".format(pd_test_data.shape[0]))\n",
        "print(\"Total number of sentences in the dev dataset: {:,}\".format(pd_dev_data[\"sentence_id\"].nunique()))\n",
        "print(\"Total words in the dev dataset: {:,}\".format(pd_dev_data.shape[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "id": "tkSr9ATX-Oq4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#print(org_list[0:12])\n",
        "#org_list_group   = extract_iob_group_entities(org_list, output_format='list', remove_duplicates=True)\n",
        "#date_list_group = extract_iob_group_entities(date_list)\n",
        "#loc_list_group  = extract_iob_group_entities(loc_list)\n",
        "#per_list_group  = extract_iob_group_entities(per_list)\n",
        "#org_list_group[:12]\n",
        "#print(only_train_ner_data[10])\n",
        "\n",
        "\n",
        "data_extract = [('Mdyə̂faʼ', 'O'), ('mtəŋláʼ', 'B-LOC'), ('shyə̂ŋkaʼ', 'I-LOC'), (',', 'O'), \n",
        "('təŋláʼ', 'B-LOC'), ('ŋkaʼ', 'I-LOC'), ('gə́', 'O'), ('təŋláʼ', 'O'), \n",
        "('Adamáwǎ', 'B-LOC'), ('kuʼ', 'O'), ('dəŋ', 'O'), ('é', 'O'), ('.', 'O'), ('.', 'O')]\n",
        "#detect_iob_tag_position(data_extract)\n",
        "\n",
        "#results_augment = augment_sentence(sent, list_ent_aug=ents_groups)\n",
        "#for sent in results_augment: print('\\t', \" \".join([i[0] for i in sent]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "id": "Poc9dwkp-Oq5",
        "outputId": "69484def-9051-49d1-a53d-9d115c91d6c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train data: 1771 \n",
            " Augmented data: 1219850\n"
          ]
        }
      ],
      "source": [
        "augmented_train_ner_data = augment_ner_iob_data(only_train_ner_data)\n",
        "\n",
        "print(f' Train data: {len(only_train_ner_data)} \\n Augmented data: {len(augmented_train_ner_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [],
        "id": "7R14kz-D-Oq5",
        "outputId": "88d393a4-4a60-4bb4-d969-aa4863172293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          sentence_id      word iob_tag\n",
              "0                   1         A       O\n",
              "1                   1     shə́ŋ       O\n",
              "2                   1      gaə́       O\n",
              "3                   1        DG       O\n",
              "4                   1  Cameroon   B-ORG\n",
              "...               ...       ...     ...\n",
              "20391456      1906795       pú       O\n",
              "20391457      1906795    pútə́       O\n",
              "20391458      1906795        é       O\n",
              "20391459      1906795        lə       O\n",
              "20391460      1906795         .       O\n",
              "\n",
              "[20391461 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-972a8f1a-d3e3-43c0-9528-8af6305c94fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>word</th>\n",
              "      <th>iob_tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>shə́ŋ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>gaə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>DG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Cameroon</td>\n",
              "      <td>B-ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20391456</th>\n",
              "      <td>1906795</td>\n",
              "      <td>pú</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20391457</th>\n",
              "      <td>1906795</td>\n",
              "      <td>pútə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20391458</th>\n",
              "      <td>1906795</td>\n",
              "      <td>é</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20391459</th>\n",
              "      <td>1906795</td>\n",
              "      <td>lə</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20391460</th>\n",
              "      <td>1906795</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20391461 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-972a8f1a-d3e3-43c0-9528-8af6305c94fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-972a8f1a-d3e3-43c0-9528-8af6305c94fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-972a8f1a-d3e3-43c0-9528-8af6305c94fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "pd_augment_ner_iob_data = list_to_pd_format(augmented_train_ner_data)\n",
        "pd_augment_ner_iob_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GA08TfI-Oq5"
      },
      "source": [
        "# Analyzing data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "id": "RdZGj3Qy-Oq6"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JL9ZTplb-Oq6"
      },
      "outputs": [],
      "source": [
        "# quantification des tags/phrases dans le corpus d'entrainement\n",
        "# credit: https://github.com/6chaoran/jupyter-summarytools\n",
        "from summarytools import dfSummary, tabset\n",
        "\n",
        "tabset({\n",
        "    \"train_data\": dfSummary(pd_augment_ner_iob_data).render(),\n",
        "    \"test_data\": dfSummary(pd_test_data).render(),\n",
        "    \"dev_data\": dfSummary(pd_dev_data).render()\n",
        "})\n",
        "\n",
        "# Un controle visuel des données et ce tableau montre clairement qu'il y a \n",
        "# des problématiques de cohérence d'annotations avec l'usage du format IOB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ygOENNvA-Oq6"
      },
      "outputs": [],
      "source": [
        "\n",
        "pd_augment_ner_iob_data[pd_augment_ner_iob_data.iob_tag != 'O'][\"iob_tag\"]\\\n",
        ".value_counts().plot(kind=\"bar\", figsize=(10,5))\n",
        "\n",
        "# On remarque ici que le tag date représente 40% des tags (en position I ou B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": [],
        "id": "m2EE2F3J-Oq6",
        "outputId": "eee3b35a-b61d-4ca4-b749-3469e98dad2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La phrase la plus longue contient 59 mots.\n",
            "ID de la plus longue phrase 118931.\n",
            "\n",
            "La phrase la plus longue du corpus est:\n",
            " Nwə yə pú kə yɔ̌ Fəládɛ̂ dɔ̂lɔ̌ Pásə́kà Tyə́ʼ kwɔ̂ʼ kɛ́bəŋ Yésô Tyə́ʼ fîʼ zhwenyə dɔ̂lɔ̌ Tyə́ʼ kwɔ̂ʼ kɛ́bəŋ Malyâ Krǐsì Lamadâŋ Laʼnwə mjʉ̀jʉ̀ Tá lə Ondo André Marie ( Mle 599 438 - T ) Tâdyə̌ mkətú' bə ě tʉɔthə́ bí pə dzə̌ é nə́ŋ cyə̂ mnə́ hɔ̂ bí nə́ pîŋ pú a fa' yə pǐŋ lə .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "word_counts = pd_augment_ner_iob_data.groupby(\"sentence_id\")[\"word\"].agg([\"count\"])\n",
        "word_counts = word_counts.rename(columns={\"count\": \"Word count\"})\n",
        "#word_counts.hist(bins=25, figsize=(8,6))\n",
        "\n",
        "# On constate que le nombre de mots moyens par phrase est de 15\n",
        "MAX_LENGTH_SENTENCE = word_counts.max()[0]\n",
        "print(\"La phrase la plus longue contient {} mots.\".format(MAX_LENGTH_SENTENCE))\n",
        "longest_sentence_id = word_counts[word_counts[\"Word count\"]==MAX_LENGTH_SENTENCE].index[0]\n",
        "print(\"ID de la plus longue phrase {}.\".format(longest_sentence_id))\n",
        "longest_sentence = pd_augment_ner_iob_data[pd_augment_ner_iob_data[\"sentence_id\"]==longest_sentence_id][\"word\"].str.cat(sep=' ')\n",
        "print(f\"\\nLa phrase la plus longue du corpus est:\\n {longest_sentence}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [],
        "id": "rfcuz-OX-Oq7",
        "outputId": "cec05daa-edfd-4eed-dffb-ea2259488757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de mots uniques: 6176\n",
            "Nombre de tags uniques : 9\n"
          ]
        }
      ],
      "source": [
        "all_words = list(set(pd_augment_ner_iob_data[\"word\"].values))\n",
        "all_tags = list(set(pd_augment_ner_iob_data[\"iob_tag\"].values))\n",
        "\n",
        "print(\"Nombre de mots uniques: {}\".format(pd_augment_ner_iob_data[\"word\"].nunique()))\n",
        "print(\"Nombre de tags uniques : {}\".format(pd_augment_ner_iob_data[\"iob_tag\"].nunique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu3GNuDX-Oq7"
      },
      "source": [
        "# Features engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": [],
        "id": "ybFUFTwx-Oq7"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import re\n",
        "from datetime import datetime\n",
        "import string\n",
        "import math\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": [],
        "id": "uJ33Z586-Oq7",
        "outputId": "ad2d04d6-27c8-4385-c4c1-a1d459e53e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['̂', 'è', 'ǒ', 'œ', 'ɔ', 'ú', '̌', 'û', 'ç', 'ə', 'ǔ', '᷆', 'à', 'ɛ', 'ï', 'ʉ', 'â', 'ʼ', 'ɨ', 'á', 'ó', 'ô', 'ǝ', 'ǐ', 'ê', 'ꞌ', 'ì', 'í', '᷅', 'î', 'é', 'ɓ', 'ŋ', 'ě', 'ɑ', '̣', '°', 'ş', 'ò', 'ù', 'ͻ', 'ë', '̀', '̧', 'ᵾ', 'ǎ', '©', '́']\n"
          ]
        }
      ],
      "source": [
        "words_caracters = set([y.lower() for x in all_words for y in x])\n",
        "all_caracters = string.punctuation+string.ascii_letters+string.digits+''\n",
        "tone_caracters = list(set([x for x in words_caracters if x not in all_caracters]))\n",
        "cpm_search = re.compile(str(tone_caracters))\n",
        "print(tone_caracters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [],
        "id": "tD3I9TZd-Oq8",
        "outputId": "8540757d-64a6-4d78-fb87-8b12f4b35337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['n', 't', 'â', 'm', 'g', 'ǒ']\n",
            "['̂', 'è', 'ǒ', 'œ', 'ɔ', 'ú', '̌', 'û', 'ç', 'ə', 'ǔ', '᷆', 'à', 'ɛ', 'ï', 'ʉ', 'â', 'ʼ', 'ɨ', 'á', 'ó', 'ô', 'ǝ', 'ǐ', 'ê', 'ꞌ', 'ì', 'í', '᷅', 'î', 'é', 'ɓ', 'ŋ', 'ě', 'ɑ', '̣', '°', 'ş', 'ò', 'ù', 'ͻ', 'ë', '̀', '̧', 'ᵾ', 'ǎ', '©', '́'] ́ ̄ ̀ ̌ ̂ \n",
            "6 2 2 ---̀ ̀\n"
          ]
        }
      ],
      "source": [
        "bantou_tones = [f\"{x} \" for x in \" ́̄̀̌̂\" if x != \" \"]\n",
        "string_tones = \"\".join(bantou_tones)\n",
        "tones_search = re.compile(string_tones)\n",
        "print([x for x in \"ntâmgǒ\"])\n",
        "print(tone_caracters, string_tones)\n",
        "\n",
        "bantou_letters = string.ascii_letters+\"ǝɔᵾɓɨşœɑʉɛɗŋøẅëïə\"\n",
        "\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    #print([x for x in nfkd_form if x not in string.ascii_letters])\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode('utf8')\n",
        "\n",
        "def extract_tone(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    #print([x for x in nfkd_form if x not in string.ascii_letters])\n",
        "    tones = [x for x in nfkd_form if x not in bantou_letters]\n",
        "    return \" \".join(tones)\n",
        "\n",
        "def number_tone_word(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    len_tone_str = len([x for x in nfkd_form if x not in bantou_letters])\n",
        "    return len_tone_str\n",
        "\n",
        "def word_decomposition(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    word_decomp = \" \".join([x for x in nfkd_form ])\n",
        "    return word_decomp\n",
        "\n",
        "def compare_two_words(input_str1, input_str2):\n",
        "    nfkd_form1 = unicodedata.normalize('NFKD', input_str1)\n",
        "    nfkd_form2 = unicodedata.normalize('NFKD', input_str2)\n",
        "    len_tone_str1 = len([x for x in nfkd_form1 if x not in bantou_letters])\n",
        "    len_tone_str2 = len([x for x in nfkd_form2 if x not in bantou_letters])\n",
        "    only_ascii = nfkd_form1.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode('utf8')\n",
        "\n",
        "non_tone = remove_accents(\"fə̀fə̀\")\n",
        "print(len(\"fə̀fə̀\"), len(non_tone), number_tone_word(\"fə̀fə̀\"), \n",
        "      \"---\"+extract_tone(\"fə̀fə̀\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": [],
        "id": "oTLG98FL-Oq8"
      },
      "outputs": [],
      "source": [
        "# l'ajout des tags suivants au mot courant améliore signficativement le modèle\n",
        "# l'ajout des informations sur les tons\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    tagword = sent[i][1]\n",
        "    len_tone = number_tone_word(word)\n",
        "    len_word = len(word) / 2\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word': word,\n",
        "        'word.tones': extract_tone(word),\n",
        "        'word.normalized': unicodedata.normalize('NFKD', word),\n",
        "        #'word.letters': word_decomposition(word),\n",
        "        'word.position': i,\n",
        "        #'word[:3]': word[:3], impacte négativement les résultats\n",
        "        #'word[:2]': word[:2],\n",
        "        #'word[-3:]': word[-3:],\n",
        "        #'word[-2:]': word[-2:],\n",
        "        #'word.middle_start': word[:int(len_word)],\n",
        "        #'word.middle_end': word[int(len_word):],\n",
        "        'word.has_hyphen': '-' in word,\n",
        "        #'word.unaccent': remove_accents(word),\n",
        "        'word.lower()': word.lower(),\n",
        "        'word.start_with_capital': word[0].isupper(),\n",
        "        'word.have_tone': True if len_tone>0 else False,\n",
        "        #'word.len_tones': len_tone,\n",
        "        'word.ispunctuation': (word in string.punctuation),\n",
        "        'word.isdigit()': word.isdigit()\n",
        "    }\n",
        "    if word == '.': features['EOS'] = True\n",
        "\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        tagword1 = sent[i-1][1]\n",
        "        len_tone1 = number_tone_word(word1)\n",
        "        len_word1 = len(word1) / 2\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:word.position': i-1,\n",
        "            '-1:word.letters': word_decomposition(word1),\n",
        "            '-1:word.normalized': unicodedata.normalize('NFKD', word1),\n",
        "            '-1:word.start_with_capital': word1[0].isupper(),\n",
        "            '-1:len(word1)': len(word1),\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            #'-1:word.tag()': tagword1,\n",
        "            #'-1:word.unaccent': remove_accents(word1),\n",
        "            #'-1:word.middle_start': word1[:int(len_word1)],\n",
        "            #'-1:word.middle_end': word1[int(len_word1):],\n",
        "            #'-1:word.have_tone': True if len_tone1>0 else False,\n",
        "            #'-1:word.len_tones': len_tone1,\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:word.ispunctuation': (word1 in string.punctuation)\n",
        "        })\n",
        "        if tagword not in ['B-ORG','B-LOC']: features.update({'-1:word.tag()': tagword1})\n",
        "    else: features['BOS'] = True\n",
        "    \n",
        "    \"\"\"\n",
        "    if i > 1:\n",
        "        word2 = sent[i-2][0]\n",
        "        tagword2 = sent[i-2][1]\n",
        "        len_tone2 = number_tone_word(word2)\n",
        "        len_word2 = len(word2) / 2\n",
        "        features.update({\n",
        "            '-2:word': word2,\n",
        "            '-2:word.position': i-2,\n",
        "            '-2:word.letters': word_decomposition(word2),\n",
        "            '-2:word.normalized': unicodedata.normalize('NFKD', word2),\n",
        "            '-2:word.start_with_capital': word2[0].isupper(),\n",
        "            '-2:len(word2)': len(word2),\n",
        "            '-2:word.lower()': word2.lower(),\n",
        "            '-2:word.tag()': tagword2,\n",
        "            #'-2:word.unaccent': remove_accents(word2),\n",
        "            #'-2:word.middle_start': word2[:int(len_word2)],\n",
        "            #'-2:word.middle_end': word2[int(len_word2):],\n",
        "            #'-2:word.have_tone': True if len_tone2>0 else False,\n",
        "            #'-2:word.len_tones': len_tone2,\n",
        "            '-2:word.isdigit()': word2.isdigit(),\n",
        "            '-2:word.ispunctuation': (word2 in string.punctuation)\n",
        "        })\n",
        "\n",
        "    if i > 2:\n",
        "        word3 = sent[i-3][0]\n",
        "        tagword3 = sent[i-3][1]\n",
        "        len_tone3 = number_tone_word(word3)\n",
        "        len_word3 = len(word3) / 2\n",
        "        features.update({\n",
        "            '-3:word': word3,\n",
        "            '-3:word.position': i+3,\n",
        "            '-3:word.letters': word_decomposition(word3),\n",
        "            '-3:word.normalized': unicodedata.normalize('NFKD', word3),\n",
        "            '-3:word.start_with_capital': word3[0].isupper(),\n",
        "            '-3:len(word3)': len(word3),\n",
        "            '-3:word.tag()': tagword3,\n",
        "            #'-3:word.lower()': word2.lower(),\n",
        "            #'-3:word.unaccent': remove_accents(word2),\n",
        "            #'-3:word.middle_start': word2[:int(len_word2)],\n",
        "            #'-3:word.middle_end': word2[int(len_word2):],\n",
        "            #'-3:word.have_tone': True if len_tone2>0 else False,\n",
        "            #'-3:word.len_tones': len_tone2,\n",
        "            '-3:word.isdigit()': word3.isdigit(),\n",
        "            '-3:word.ispunctuation': (word3 in string.punctuation)\n",
        "        })\n",
        "    \"\"\"\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        tagword1 = sent[i+1][1]\n",
        "        len_tone1 = number_tone_word(word1)\n",
        "        len_word1 = len(word1) / 2\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:word.position': i+1,\n",
        "            '+1:word.letters': word_decomposition(word1),\n",
        "            '+1:word.normalized': unicodedata.normalize('NFKD', word1),\n",
        "            '+1:word.start_with_capital': word1[0].isupper(),\n",
        "            '+1:len(word2)': len(word1),\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            #'+1:word.tag()': tagword1,\n",
        "            #'+1:word.unaccent': remove_accents(word1),\n",
        "            #'+1:word.middle_start': word1[:int(len_word1)],\n",
        "            #'+1:word.middle_end': word1[int(len_word1):],\n",
        "            #'+1:word.have_tone': True if len_tone1>0 else False,\n",
        "            #'+1:word.len_tones': len_tone1,\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:word.ispunctuation': (word1 in string.punctuation)\n",
        "        })\n",
        "    \"\"\" \n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2][0]\n",
        "        tagword2 = sent[i+2][1]\n",
        "        len_tone2 = number_tone_word(word2)\n",
        "        len_word2 = len(word2) / 2\n",
        "        features.update({\n",
        "            '+2:word': word2,\n",
        "            '+2:word.position': i+2,\n",
        "            '+2:word.letters': word_decomposition(word2),\n",
        "            '+2:word.normalized': unicodedata.normalize('NFKD', word2),\n",
        "            '+2:word.start_with_capital': word2[0].isupper(),\n",
        "            '+2:len(word2)': len(word2),\n",
        "            '+2:word.tag()': tagword2,\n",
        "            #'+2:word.lower()': word2.lower(),\n",
        "            #'+2:word.unaccent': remove_accents(word2),\n",
        "            #'+2:word.middle_start': word2[:int(len_word2)],\n",
        "            #'+2:word.middle_end': word2[int(len_word2):],\n",
        "            #'+2:word.have_tone': True if len_tone2>0 else False,\n",
        "            #'+2:word.len_tones': len_tone2,\n",
        "            '+2:word.isdigit()': word2.isdigit(),\n",
        "            '+2:word.ispunctuation': (word2 in string.punctuation)\n",
        "        })\n",
        "        \n",
        "    if i < len(sent)-3:\n",
        "        word3 = sent[i+3][0]\n",
        "        tagword3 = sent[i+3][1]\n",
        "        len_tone3 = number_tone_word(word3)\n",
        "        len_word3 = len(word3) / 2\n",
        "        features.update({\n",
        "            '+3:word': word3,\n",
        "            '+3:word.position': i+3,\n",
        "            '+3:word.letters': word_decomposition(word3),\n",
        "            '+3:word.normalized': unicodedata.normalize('NFKD', word3),\n",
        "            '+3:word.start_with_capital': word3[0].isupper(),\n",
        "            '+3:len(word3)': len(word3),\n",
        "            '+3:word.tag()': tagword3,\n",
        "            #'+2:word.lower()': word2.lower(),\n",
        "            #'+2:word.unaccent': remove_accents(word2),\n",
        "            #'+2:word.middle_start': word2[:int(len_word2)],\n",
        "            #'+2:word.middle_end': word2[int(len_word2):],\n",
        "            #'+2:word.have_tone': True if len_tone2>0 else False,\n",
        "            #'+2:word.len_tones': len_tone2,\n",
        "            '+3:word.isdigit()': word3.isdigit(),\n",
        "            '+3:word.ispunctuation': (word3 in string.punctuation)\n",
        "        })\n",
        "    \"\"\"\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [word[1] for word in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [word[0] for word in sent]\n",
        "\n",
        "def format_data(csv_data):\n",
        "    sents = []\n",
        "    \"\"\"for i in range(len(csv_data)):\n",
        "        if math.isnan(float(csv_data.iloc[i, 0])): continue\n",
        "        elif csv_data.iloc[i, 0] == 1.0:\n",
        "            sents.append([[csv_data.iloc[i, 1], csv_data.iloc[i, 2]]])\n",
        "        else:\n",
        "            try: sents[-1].append([csv_data.iloc[i, 1], csv_data.iloc[i, 2]])\n",
        "            except: print('...', csv_data.iloc[i, 2])\n",
        "    for sent in sents:\n",
        "        for i, word in enumerate(sent):\n",
        "            if type(word[0]) != str:\n",
        "                del sent[i]\"\"\"\n",
        "    return csv_data\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": [],
        "id": "ZnG_xv72-Oq9",
        "outputId": "763211de-1de3-4c77-946c-d6613448f9c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20391461 15924\n"
          ]
        }
      ],
      "source": [
        "print(len(pd_augment_ner_iob_data), len(pd_test_data))\n",
        "\n",
        "#train_sents = format_data(pd_train_data)\n",
        "#test_sents = format_data(pd_test_data)\n",
        "#dev_sents = format_data(pd_dev_data)\n",
        "train_sents = [[word for word in sentence] for sentence in augmented_train_ner_data]\n",
        "test_sents = [[word for word in sentence] for sentence in extracted_test_data]\n",
        "dev_sents = [[word for word in sentence] for sentence in extracted_dev_data]\n",
        "\n",
        "Xtrain = [sent2features(s) for s in train_sents]\n",
        "ytrain = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "Xtest = [sent2features(s) for s in test_sents]\n",
        "ytest = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "Xdev = [sent2features(s) for s in dev_sents]\n",
        "ydev = [sent2labels(s) for s in dev_sents]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgDr0mtx-Oq9"
      },
      "outputs": [],
      "source": [
        "Xtrain[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOFiMRD2-Oq9"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P4EOYXRf-Oq9"
      },
      "outputs": [],
      "source": [
        "#import pycrfsuite\n",
        "import sklearn_crfsuite\n",
        "import math, string, re\n",
        "import scipy\n",
        "import joblib\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from itertools import chain\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycrfsuite\n"
      ],
      "metadata": {
        "id": "yVzOn47rQFuU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = pycrfsuite.Trainer(verbose=True)\n",
        "\n",
        "for xseq, yseq in zip(Xtrain, ytrain):\n",
        "    trainer.append(xseq, yseq)"
      ],
      "metadata": {
        "id": "2jN52z5GQQky"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project = \"sangkak-02-2023-aug\"\n",
        "build_date = str(datetime.now()).replace(' ','_')\n",
        "model_name = Path(f\"crf_{project}_{build_date}.model\")\n",
        "model_file = str(model_name)\n",
        "file_crf = Path(f\"crf_{build_date}.object\")\n",
        "\n",
        "print(trainer.params())\n",
        "trainer.set_params({\n",
        "    #\"algorithm\": 'lbfgs',\n",
        "    \"c1\": 0.0920512484757745,\n",
        "    \"c2\": 0.0328771171605105, \n",
        "    \"max_iterations\":100,\n",
        "    #\"verbose\":True,\n",
        "    \"num_memories\":10000,\n",
        "    \"epsilon\": 1e-3,\n",
        "    \"linesearch\": \"MoreThuente\",\n",
        "    \"max_linesearch\":100000,\n",
        "    \"delta\":1e-4,\n",
        "    #\"n_job\":-1,\n",
        "    #\"c\": 2,\n",
        "    #\"pa_type\": 2,\n",
        "    \"feature.possible_transitions\":True,\n",
        "    \"feature.possible_states\":True, \n",
        "    #\"model_filename\": model_file\n",
        "})"
      ],
      "metadata": {
        "id": "ohqw-e0jSz9l",
        "outputId": "aeeab435-b513-4be6-fbcb-d3486bc804b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['feature.minfreq', 'feature.possible_states', 'feature.possible_transitions', 'c1', 'c2', 'max_iterations', 'num_memories', 'epsilon', 'period', 'delta', 'linesearch', 'max_linesearch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(trainer.train)"
      ],
      "metadata": {
        "id": "96aazDFoUdCP",
        "outputId": "ddfa5562-1b07-4fb9-8a7c-9d7811bab6c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function train:\n",
            "\n",
            "train(...) method of pycrfsuite._pycrfsuite.Trainer instance\n",
            "    BaseTrainer.train(self, model, int holdout=-1)\n",
            "    \n",
            "    Run the training algorithm.\n",
            "    This function starts the training algorithm with the data set given\n",
            "    by :meth:`Trainer.append` method.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    model : string\n",
            "        The filename to which the trained model is stored.\n",
            "        If this value is empty, this function does not\n",
            "        write out a model file.\n",
            "    \n",
            "    holdout : int, optional\n",
            "        The group number of holdout evaluation. The\n",
            "        instances with this group number will not be used\n",
            "        for training, but for holdout evaluation.\n",
            "        Default value is -1, meaning \"use all instances for training\".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train('conll2002-esp.crfsuite')"
      ],
      "metadata": {
        "id": "MIj2cHR4RZna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fcEyIc-0-Oq9",
        "outputId": "06c5193c-7e78-4a7c-ac36-d36451c06bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading training data to CRFsuite:  37%|███▋      | 447114/1219850 [02:56<05:04, 2536.05it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-13d4a5a2d714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn_crfsuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mydev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"crf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn_crfsuite/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_dev, y_dev)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "project = \"sangkak-02-2023-aug\"\n",
        "build_date = str(datetime.now()).replace(' ','_')\n",
        "model_name = Path(f\"crf_{project}_{build_date}.model\")\n",
        "model_file = str(model_name)\n",
        "file_crf = Path(f\"crf_{build_date}.object\")\n",
        "\n",
        "params = {\n",
        "    \"algorithm\": 'lbfgs',\n",
        "    \"c1\": 0.0920512484757745,\n",
        "    \"c2\": 0.0328771171605105, \n",
        "    \"max_iterations\":100,\n",
        "    \"verbose\":True,\n",
        "    \"num_memories\":10000,\n",
        "    \"epsilon\": 1e-3,\n",
        "    \"linesearch\": \"MoreThuente\",\n",
        "    \"max_linesearch\":100000,\n",
        "    \"delta\":1e-4,\n",
        "    #n_job=-1,\n",
        "    #\"c\": 2,\n",
        "    #\"pa_type\": 2,\n",
        "    \"all_possible_states\":True,\n",
        "    \"all_possible_transitions\":True, \n",
        "    \"model_filename\": model_file\n",
        "}\n",
        "crf = sklearn_crfsuite.CRF(**params)\n",
        "\n",
        "crf.fit(Xtrain, ytrain, Xdev, ydev)    \n",
        "\n",
        "final = {\"crf\": crf, \"params\": params}\n",
        "joblib.dump(final, file_crf) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g25xXpD1-Oq-"
      },
      "outputs": [],
      "source": [
        "# obtaining metrics such as accuracy, etc. on the train set\n",
        "#crf_model = joblib.load(model_name)\n",
        "#crf, params = crf_model['crf'], crf_model['params'] \n",
        "labels = list(crf.classes_)\n",
        "labels.remove('O')\n",
        "\n",
        "ypred = crf.predict(Xtrain)\n",
        "print('- F1 score on the train set = {}'.format(metrics.flat_f1_score(ytrain, ypred, average='weighted', labels=labels, zero_division=False)))\n",
        "print('- Accuracy on the train set = {}'.format(metrics.flat_accuracy_score(ytrain, ypred)))\n",
        "\n",
        "sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
        "print(sorted_labels)\n",
        "#print('Train set classification report: \\n\\n{}'.format(metrics.flat_classification_report(ytrain, \n",
        "#ypred, labels=sorted_labels, digits=3)))\n",
        "#obtaining metrics such as accuracy, etc. on the test set\n",
        "ypred = crf.predict(Xtest)\n",
        "print('- F1 score on the test set = {}'.format(metrics.flat_f1_score(ytest, ypred, average='weighted', \n",
        "                                                labels=labels, zero_division=False)))\n",
        "print('- Accuracy on the test set = {}'.format(metrics.flat_accuracy_score(ytest, ypred)))\n",
        "print('Train set classification report: \\n\\n{}'.format(metrics.flat_classification_report(ytest, \n",
        "                            ypred, labels=sorted_labels, digits=3)))\n",
        "sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fpWrViU-Oq-"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
        "\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1dArD7ST-Oq-"
      },
      "outputs": [],
      "source": [
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
        "\n",
        "print(\"Top positive:\")\n",
        "#print_state_features(Counter(crf.state_features_).most_common(30))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common()[-40:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVhARMA2-Oq-"
      },
      "source": [
        "# Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3WUBR4W-Oq-"
      },
      "outputs": [],
      "source": [
        "scipy.stats.expon(scale=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JncBFfw4-Oq-"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"algorithm\": 'lbfgs',\n",
        "    \"max_iterations\":100,\n",
        "    \"verbose\": False,\n",
        "    #\"job\":-1,\n",
        "    \"all_possible_states\":True,\n",
        "    \"all_possible_transitions\":True, \n",
        "    \"model_filename\":model_file\n",
        "}\n",
        "crf_grill = sklearn_crfsuite.CRF(**params)\n",
        "\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.1),\n",
        "    'c2': scipy.stats.expon(scale=0.05)\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf_grill, params_space,\n",
        "                        cv=3,\n",
        "                        verbose=1,\n",
        "                        n_jobs=5,\n",
        "                        n_iter=50,\n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxwJK1dR-Oq-"
      },
      "outputs": [],
      "source": [
        "# crf = rs.best_estimator_\n",
        "print('best params:', rs.best_params_)\n",
        "print('best CV score:', rs.best_score_)\n",
        "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4sNnTjQ-Oq_"
      },
      "outputs": [],
      "source": [
        "#print(rs.cv_results_)\n",
        "_x = [s['c1'] for s in rs.cv_results_['params']]\n",
        "_y = [s['c2'] for s in rs.cv_results_['params']]\n",
        "_c = [s for s in rs.cv_results_['mean_score_time']]\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(12, 12)\n",
        "ax = plt.gca()\n",
        "ax.set_yscale('log')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlabel('C1')\n",
        "ax.set_ylabel('C2')\n",
        "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
        "    min(_c), max(_c)\n",
        "))\n",
        "\n",
        "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
        "\n",
        "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km6KCWbX-Oq_"
      },
      "outputs": [],
      "source": [
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(Xtest)\n",
        "print(metrics.flat_classification_report(\n",
        "    ytest, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02kA8FA6-Oq_"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
        "\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jajyQFfo-Oq_"
      },
      "outputs": [],
      "source": [
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
        "\n",
        "print(\"Top positive:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common(30))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "646b439550bec9cacc5e0384422c9ee78f8df74b182cfe1fc7410e07b34d6961"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}